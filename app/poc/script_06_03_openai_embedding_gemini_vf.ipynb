{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c80e99-4508-4373-b376-8087b548cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==1.93.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449e984-6263-43c1-bbba-81b2c309cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the specified path\n",
    "load_dotenv(dotenv_path='/home/sparky/.dbt/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9898aee-8a16-46e3-974b-0b3e2dc3b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f0b7f9-5f36-4671-8678-7fccb3602dca",
   "metadata": {},
   "source": [
    "## Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a69e4a-4397-483d-a455-668b7888e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = '/opt/spark/work-dir/data/exists_data/raw_embedding_demo.csv'\n",
    "\n",
    "raw_df = pl.read_csv(source=raw_path, has_header=True)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b2d49-13dc-4d8c-91ba-5b8e542ec18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_01 = raw_df.filter(pl.col(\"ecommerce_name\") == 'ecommerce_01')\n",
    "em_02 = raw_df.filter(pl.col(\"ecommerce_name\") == 'ecommerce_02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea5e07-2be8-4ba8-a52a-dfc9583b96b1",
   "metadata": {},
   "source": [
    "## Setup & Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd1c17-3ef8-4989-a3cc-abd8d89990f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure OPENAI_API_KEY environment variable is set\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266371b4-9aa0-4c28-ae09-b1d91e61cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function: generate text embeddings\n",
    "def generate_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generate embeddings for input text using OpenAI's text-embedding-3-small model.\n",
    "\n",
    "    Args:\n",
    "        text (str or list[str]): The text or list of texts to embed.\n",
    "\n",
    "    Returns:\n",
    "        list or list[list]: A list of embeddings, or a list of lists of embeddings if input was a list.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embeddings = [record.embedding for record in response.data]\n",
    "    return embeddings if isinstance(text, list) else embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91164898-86d5-4810-9bf8-d139ec22f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding cache path\n",
    "embedding_cache_path = \"embeddings_cache.pkl\"\n",
    "\n",
    "# Try to load cached embeddings\n",
    "try:\n",
    "    with open(embedding_cache_path, 'rb') as f:\n",
    "        embedding_cache = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9a33a-8a0a-4867-ba75-587380816a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function: obtain text embeddings through caching mechanism\n",
    "def embedding_from_string(string: str, embedding_cache=embedding_cache) -> list:\n",
    "    \"\"\"\n",
    "    Get embedding for given text, using cache mechanism to avoid recomputation.\n",
    "\n",
    "    Args:\n",
    "        string (str): The input text to get the embedding for.\n",
    "        embedding_cache (dict): A dictionary used as a cache for embeddings.\n",
    "\n",
    "    Returns:\n",
    "        list: The embedding vector for the input string.\n",
    "    \"\"\"\n",
    "    if string not in embedding_cache.keys():\n",
    "        embedding_cache[string] = generate_embeddings(string)\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[string]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c10c7e4-7987-4a25-89a6-aa38be2a767b",
   "metadata": {},
   "source": [
    "## Embed base product name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cafb68-1b8a-49e9-8006-1f3a0cbce81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dynamic batch size. You can adjust this value based on API limits and performance.\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "print(\"Preparing base by generating embeddings for all base products...\")\n",
    "# Get all product names from the base dataframe\n",
    "base_names = em_02.select('product_name').to_series().to_list()\n",
    "base_prices = em_02.select('sale_price').to_series().to_list()\n",
    "\n",
    "# 1. Identify which product names are new and need to be embedded.\n",
    "names_to_embed = [name for name in base_names if name not in embedding_cache]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c866cd-c99d-4848-873e-1dfd8eda45f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. If there are any new names, process them in manageable batches.\n",
    "if names_to_embed:\n",
    "    total_batches = -(-len(names_to_embed) // BATCH_SIZE)  # Ceiling division to calculate total batches\n",
    "    print(f\"Found {len(names_to_embed)} new products to embed. Processing in {total_batches} batches of up to {BATCH_SIZE} items each...\")\n",
    "    \n",
    "    # Iterate through the new names in chunks of BATCH_SIZE\n",
    "    for i in range(0, len(names_to_embed), BATCH_SIZE):\n",
    "        current_batch_num = (i // BATCH_SIZE) + 1\n",
    "        \n",
    "        # Define the current batch of names\n",
    "        batch_names = names_to_embed[i:i + BATCH_SIZE]\n",
    "        print(f\"  - Processing batch {current_batch_num}/{total_batches} ({len(batch_names)} items)...\")\n",
    "        \n",
    "        # Generate embeddings for the current batch\n",
    "        batch_embeddings = generate_embeddings(batch_names)\n",
    "        \n",
    "        # 3. Update the cache with the newly generated embeddings from this batch\n",
    "        for name, embedding in zip(batch_names, batch_embeddings):\n",
    "            embedding_cache[name] = embedding\n",
    "\n",
    "        # 4. Save the updated cache to disk after each batch. This adds robustness.\n",
    "        #    If the script fails, progress from completed batches is not lost.\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "        print(f\"  - Batch {current_batch_num} processed and cache saved.\")\n",
    "            \n",
    "    print(\"All new embeddings have been generated and cached.\")\n",
    "\n",
    "# 5. Retrieve all embeddings from the cache to ensure the list is complete and in the correct order.\n",
    "base_embeddings = [embedding_cache[name] for name in base_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4258b3b-44fa-44ca-b8e3-136dd777b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_df = {\n",
    "    'product_name': base_names,\n",
    "    'embedding': base_embeddings,\n",
    "    'sale_price': base_prices\n",
    "}\n",
    "product_embeddings_df = pl.DataFrame(data_for_df)\n",
    "product_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e38df-f5fe-45c5-baaf-dfd846caf842",
   "metadata": {},
   "source": [
    "## Find Best Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c747f7-7c2d-46a9-91b7-37877036d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c1df6-0b18-4c25-8158-3292cedc4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "gemini_client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94b05b-e185-40e3-b96f-3ca76aa9c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CORE MATCHING LOGIC (Unchanged) ---\n",
    "def find_best_match_in_context(query_name, context_embeddings_np):\n",
    "    \"\"\"\n",
    "    Finds the single most similar product in the context catalog for a given query name.\n",
    "    \"\"\"\n",
    "    # The embedding for the query product is generated here, on-the-fly.\n",
    "    query_embedding = embedding_from_string(query_name)\n",
    "    similarities = np.dot(context_embeddings_np, query_embedding) / \\\n",
    "                   (np.linalg.norm(context_embeddings_np, axis=1) * np.linalg.norm(query_embedding))\n",
    "    best_index = np.argmax(similarities)\n",
    "    return best_index, similarities[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe5237-af40-44dd-a395-e4fdcc13a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. NEW GEMINI-BASED VERIFICATION LOGIC (Unchanged) ---\n",
    "def verify_matches_with_gemini(product_pairs: list[dict]):\n",
    "    \"\"\"\n",
    "    Sends a list of product pairs to the Gemini API for verification.\n",
    "\n",
    "    Args:\n",
    "        product_pairs (list[dict]): A list of dictionaries, where each dict contains\n",
    "                                    'query_product_name' and 'candidate_product_name'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of verification results from the LLM.\n",
    "    \"\"\"\n",
    "    if not product_pairs:\n",
    "        return []\n",
    "\n",
    "    # Construct the prompt with up to 10 pairs\n",
    "    prompt_lines = [\n",
    "        \"You are a precise product matching assistant. For each numbered item below, determine if the 'Base Product' and the 'Candidate Product' are the same item.\",\n",
    "        \"Your response must be a valid JSON object containing a single key 'verifications', which holds a list of JSON objects.\",\n",
    "        \"Each object in the list must correspond to an item in the input and contain a single key: 'match_found' with a boolean value (true or false).\",\n",
    "        \"Ensure the order of your response list matches the order of the products provided.\",\n",
    "        \"\\n--- PRODUCT PAIRS ---\"\n",
    "    ]\n",
    "\n",
    "    for i, pair in enumerate(product_pairs, 1):\n",
    "        prompt_lines.append(f\"\\n{i}. Base Product: '{pair['query_product_name']}'\")\n",
    "        prompt_lines.append(f\"   Candidate Product: '{pair['candidate_product_name']}'\")\n",
    "\n",
    "    prompt = \"\\n\".join(prompt_lines)\n",
    "\n",
    "    try:\n",
    "        # 3. Call the generate_content method with the prompt.\n",
    "        print(f\"\\nSending {len(product_pairs)} pairs to Gemini for verification...\")\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model='gemini-2.5-flash-lite-preview-06-17',\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                \"response_mime_type\":\"application/json\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # --- FIXED SECTION END ---\n",
    "        \n",
    "        # The response.text already contains the JSON string\n",
    "        result_json = json.loads(response.text)\n",
    "        return result_json.get('verifications', [])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while calling the Gemini API: {e}\")\n",
    "        # Return a list of 'False' results to prevent crashing the main loop\n",
    "        return [{'match_found': False} for _ in product_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af615d-64cb-436a-84c0-9b5e83a67efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. MODIFIED MAIN PROCESSING SCRIPT (Unchanged) ---\n",
    "\n",
    "# Define the batch size for each API call\n",
    "API_CALL_SIZE = 10\n",
    "\n",
    "# df_query corresponds to the new products to be checked.\n",
    "# df_base corresponds to the existing product catalog.\n",
    "df_query = em_01\n",
    "df_base = product_embeddings_df\n",
    "base_embeddings_np = np.array(df_base['embedding'].to_list())\n",
    "\n",
    "results_list = []\n",
    "# This correctly iterates through your 'em_01' DataFrame row by row.\n",
    "query_rows = list(df_query.iter_rows(named=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ccaa25-5566-49f5-a645-fb6f5118588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "query_rows = list(df_query.iter_rows(named=True))\n",
    "\n",
    "print(f\"Starting product matching in sequential batches of {BATCH_SIZE}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333703b6-f8ca-42cc-8dc8-62f245d0841a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Starting product matching in chunks of {API_CALL_SIZE}...\")\n",
    "\n",
    "# Process the query dataframe in sequential chunks\n",
    "for i in range(0, len(query_rows), API_CALL_SIZE):\n",
    "    product_chunk = query_rows[i:i + API_CALL_SIZE]\n",
    "    chunk_num = (i // API_CALL_SIZE) + 1\n",
    "    print(f\"\\n--- Processing Chunk {chunk_num} ({len(product_chunk)} products) ---\")\n",
    "    \n",
    "    chunk_data_for_verification = []\n",
    "    chunk_original_data = []\n",
    "\n",
    "    for row in product_chunk:\n",
    "        query_product_name = row.get('product_name')\n",
    "        if not query_product_name:\n",
    "            continue\n",
    "\n",
    "        # Find the best candidate match from the base catalog\n",
    "        match_index, similarity = find_best_match_in_context(query_product_name, base_embeddings_np)\n",
    "        matched_product_info = df_base.row(match_index, named=True)\n",
    "        \n",
    "        chunk_data_for_verification.append({\n",
    "            'query_product_name': query_product_name,\n",
    "            'candidate_product_name': matched_product_info['product_name']\n",
    "        })\n",
    "        \n",
    "        chunk_original_data.append({\n",
    "            'original_row': row,\n",
    "            'match_info': matched_product_info,\n",
    "            'similarity': similarity\n",
    "        })\n",
    "\n",
    "    # Call the Gemini API for the entire chunk\n",
    "    llm_results = verify_matches_with_gemini(chunk_data_for_verification)\n",
    "\n",
    "    if len(llm_results) != len(chunk_original_data):\n",
    "        print(f\"Warning: Mismatch between sent items ({len(chunk_original_data)}) and received results ({len(llm_results)}). Skipping chunk.\")\n",
    "        continue\n",
    "\n",
    "    for original_data, llm_result in zip(chunk_original_data, llm_results):\n",
    "        try:\n",
    "            is_match = llm_result.get('match_found', False)\n",
    "        except (AttributeError, TypeError):\n",
    "            print(f\"Could not parse LLM result: {llm_result}. Treating as no match.\")\n",
    "            is_match = False\n",
    "\n",
    "        query_row = original_data['original_row']\n",
    "        match_info = original_data['match_info']\n",
    "        similarity_score = original_data['similarity']\n",
    "\n",
    "        results_list.append({\n",
    "            'base_product_name (from line)': query_row['product_name'],\n",
    "            'base_product_name (from watson)': match_info['product_name'],\n",
    "            'price_from_line': query_row['sale_price'],\n",
    "            'price_from_watson': match_info['sale_price'],\n",
    "            'similarity': float(similarity_score),\n",
    "            'verified_match': is_match\n",
    "        })\n",
    "    \n",
    "    print(f\"Finished processing Chunk {chunk_num}.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- All chunks processed. Final results compiled. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2e965-70d2-49ea-9dd0-54aaf3997ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FINAL STEP: CONVERT RESULTS TO DATAFRAME AND SAVE TO CSV (Unchanged) ---\n",
    "if results_list:\n",
    "    print(\"Converting results to a Polars DataFrame...\")\n",
    "    results_df = pl.DataFrame(results_list)\n",
    "\n",
    "    output_filename = \"product_matching_results_gemini.csv\"\n",
    "    try:\n",
    "        results_df.write_csv(output_filename)\n",
    "        print(f\"\\nSuccessfully saved {len(results_df)} results to '{output_filename}'\")\n",
    "        print(\"Final DataFrame head:\")\n",
    "        print(results_df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to CSV: {e}\")\n",
    "else:\n",
    "    print(\"No results were generated to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e00678-7cc8-446e-a74d-946d8f518c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
